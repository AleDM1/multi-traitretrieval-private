{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49558136-156b-48ca-ad0c-d785d672eb0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "from data_module_F import *\n",
    "from model_module_F import *\n",
    "from feature_module_F import *\n",
    "from evaluation_module_F import *\n",
    "from tensorflow.keras.models import model_from_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8e3cdbf-2282-416b-a394-794897d0fed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "######## GPU RAM memory ##########\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  # Restrict TensorFlow to only allocate 15*1GB of memory on the first GPU\n",
    "    try:\n",
    "        tf.config.set_logical_device_configuration(\n",
    "            gpus[0],\n",
    "            [tf.config.LogicalDeviceConfiguration(memory_limit=1024*5)])\n",
    "        logical_gpus = tf.config.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        # Virtual devices must be set before GPUs have been initialized\n",
    "            print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8fb9eb4a-08b3-4121-a377-c1c35147941a",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'dataset/' ## data path\n",
    "exp = 'test_dl' ## experiment name \n",
    "seed = 1 ### the number of fold (from 1 to 5)\n",
    "route = './models/'####Inset ## experiments folder \n",
    "\n",
    "epochs = 300\n",
    "lr = 0.0005\n",
    "kind = 'efficientNet' ## type of model architecture other options can be None: simple CNN or 'resnet'\n",
    "\n",
    "dir_n = route + '{}_{}/'.format(exp,seed) ## Specific experiment dir to be created\n",
    "create_path(dir_n)\n",
    "\n",
    "tf.random.set_seed(155) ### seed for the experiement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c094e3db-f821-41c9-b946-699c2abcafe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "db_train, X_train, y_train = read_db(path + 'fillCV_{}.csv'.format(seed),sp=True)\n",
    "db_test, X_test, y_test = read_db(path + 'testCV_{}.csv'.format(seed),sp=True)\n",
    "\n",
    "fill = db_train.copy()\n",
    "\n",
    "samp_w_tr_all = pd.read_csv(path + 'samp_w_tr_{}.csv'.format(seed)).drop(['Unnamed: 0'],axis=1).loc[:,'0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "919c9351-6f79-4a9c-aef8-faf014b1558f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "446/446 [==============================] - 16s 32ms/step - loss: 53.1506 - root_mean_squared_error: 3.7392 - r_square: -12.9334 - val_loss: 25.8609 - val_root_mean_squared_error: 0.7764 - val_r_square: 0.3776\n",
      "Epoch 2/2\n",
      "446/446 [==============================] - 14s 31ms/step - loss: 12.8271 - root_mean_squared_error: 0.5188 - r_square: 0.7318 - val_loss: 12.8275 - val_root_mean_squared_error: 0.5295 - val_r_square: 0.7105\n",
      "Epoch 1/2\n",
      "348/348 [==============================] - 12s 33ms/step - loss: 90.4471 - root_mean_squared_error: 5.5747 - r_square: -30.0723 - val_loss: 37.2418 - val_root_mean_squared_error: 0.9410 - val_r_square: 0.1082\n",
      "Epoch 2/2\n",
      "348/348 [==============================] - 11s 32ms/step - loss: 25.2223 - root_mean_squared_error: 0.7593 - r_square: 0.4236 - val_loss: 25.0655 - val_root_mean_squared_error: 0.7537 - val_r_square: 0.4278\n"
     ]
    }
   ],
   "source": [
    "test = pd.DataFrame()\n",
    "\n",
    "path_g = dir_n + 'all_models/'\n",
    "create_path(path_g)\n",
    "\n",
    "gap_fil = fill.copy()\n",
    "\n",
    "for i in range(len(Traits[:2])):\n",
    "\n",
    "    train_x, train_y = data_prep('400', gap_fil, Traits, i=i)\n",
    "    samp_w_tr = samp_w_tr_all.loc[train_x.index]\n",
    "    \n",
    "        ##############\n",
    "    scaler_list = save_scaler(train_y, save=True, dir_n = dir_n, k = i+1)\n",
    "\n",
    "    val_x = train_x.sample(frac = 0.1,random_state = seed)\n",
    "    val_y = train_y.loc[val_x.index,:]\n",
    "    samp_w_val = pd.DataFrame(samp_w_tr).sample(frac = 0.1,random_state = seed)\n",
    "\n",
    "    if(samp_w_tr is not None):\n",
    "        if (samp_w_tr.sum().sum() !=0):    \n",
    "            train_ds = dataset(train_x.drop(val_x.index), train_y.drop(val_y.index), pd.DataFrame(samp_w_tr).drop(samp_w_val.index), scaler_list, Traits, shuffle=True,augment=True)\n",
    "            test_ds = dataset(val_x, val_y, samp_w_val, scaler_list, Traits)\n",
    "    else:\n",
    "        train_ds = dataset(train_x.drop(val_x.index), train_y.drop(val_y.index), None, scaler_list, Traits, shuffle=True,augment=True)\n",
    "        test_ds = dataset(val_x, val_y, None, scaler_list, Traits)\n",
    "\n",
    "        ##### Model definition  and taraining #######\n",
    "    input_shape = train_x.shape[1]\n",
    "    output_shape = train_y.shape[1]\n",
    "\n",
    "    best_model = model_definition(input_shape, output_shape,lr= lr, kind=kind)\n",
    "\n",
    "    # callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10,restore_best_weights=True)\n",
    "\n",
    "    checkpoint_path = path_g + 'checkpoint_{}'.format(i)\n",
    "    create_path(checkpoint_path)\n",
    "\n",
    "    model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath = checkpoint_path + \"/epoch{epoch:02d}-val_root_mean_squared_error{val_root_mean_squared_error:.2f}.hdf5\",\n",
    "    save_weights_only=True,\n",
    "    monitor = 'val_root_mean_squared_error',\n",
    "    mode='min',\n",
    "    save_best_only=True)\n",
    "\n",
    "    his = best_model.fit(train_ds,\n",
    "                    validation_data = test_ds,\n",
    "                    epochs = epochs,\n",
    "                    verbose=1, callbacks = [model_checkpoint_callback])\n",
    "\n",
    "    val_acc_per_epoch = his.history['val_root_mean_squared_error']\n",
    "    best_epoch = val_acc_per_epoch.index(min(val_acc_per_epoch)) + 1    \n",
    "\n",
    "    path_trial = path_g + \"Model{}.json\".format(i)\n",
    "    path_best = checkpoint_path + \"/epoch{0:02d}-val_root_mean_squared_error{1:.2f}.hdf5\".format(best_epoch,min(val_acc_per_epoch))\n",
    "    path_w = path_g + 'Trial{}_weights.h5'.format(i)\n",
    "    save_model(best_model, path_trial, path_best, path_w)\n",
    "    \n",
    "    ######## Evaluation ##########\n",
    "    test_x, test_y = data_prep('400', db_test, Traits, i=i)\n",
    "\n",
    "    pred = scaler_list.inverse_transform(best_model.predict(test_x))\n",
    "    pred_df = pd.DataFrame(pred, columns = test_y.columns+ ' Predictions')\n",
    "    # pred_df.to_csv(path_g + 'Predictions_{}.csv'.format(i))\n",
    "\n",
    "    obs_pf = pd.DataFrame(test_y)\n",
    "    # obs_pf.to_csv(path_g + 'observations_{}.csv'.format(i))\n",
    "\n",
    "    test = pd.concat([test,all_scores(Traits[i:i+1],Traits,obs_pf, pred_df,None)],axis=1)\n",
    "\n",
    "# test.to_csv(path_g + 'Global_all.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5d2f9722-a733-429c-ba38-66d4a946ce5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LMA (g/m²)</th>\n",
       "      <th>N content (mg/cm²)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>r2_score</th>\n",
       "      <td>0.671747</td>\n",
       "      <td>0.316057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RMSE</th>\n",
       "      <td>38.879321</td>\n",
       "      <td>0.076818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nRMSE (%)</th>\n",
       "      <td>10.778344</td>\n",
       "      <td>16.026837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MAE</th>\n",
       "      <td>23.207871</td>\n",
       "      <td>0.052986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bias</th>\n",
       "      <td>2.787777</td>\n",
       "      <td>-0.009210</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           LMA (g/m²)  N content (mg/cm²)\n",
       "r2_score     0.671747            0.316057\n",
       "RMSE        38.879321            0.076818\n",
       "nRMSE (%)   10.778344           16.026837\n",
       "MAE         23.207871            0.052986\n",
       "Bias         2.787777           -0.009210"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
